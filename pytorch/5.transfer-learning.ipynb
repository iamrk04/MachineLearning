{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Format of the input: **[batch_size, no_of_channels, height, width]**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU information\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import scripts.data_setup as data_setup\n",
    "import scripts.engine as engine\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data ready (turn into tensor)\n",
    "Our dataset is a subset of the Food101 dataset. Food101 starts 101 different classes of food and 1000 images per class (750 training, 250 testing). Our dataset starts with 3 classes of food and only 10% of the images (~75 training, 25 testing).\n",
    "\n",
    "Why do this?\n",
    "- When starting out ML projects, it's important to try things on a small scale and then increase the scale when necessary.\n",
    "- The whole point is to speed up how fast you can experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.get_data as get_data\n",
    "import os\n",
    "\n",
    "\n",
    "url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "image_path = os.path.join(os.getcwd(), \"data\", \"pizza_steak_sushi\")\n",
    "get_data.download_data(url=url, save_path_str=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check details about data\n",
    "\n",
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "    \"\"\"Walks through dir_path returning its contents.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Setup train and testing paths\n",
    "image_path = Path(image_path)\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data (randomly take some images)\n",
    "\n",
    "import random \n",
    "from PIL import Image\n",
    "\n",
    "# Set seed\n",
    "# random.seed(RANDOM_SEED)\n",
    "\n",
    "# 1. Get all image paths \n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "# 2. Pick a random image path\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# 4. Open image\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# 5. Print metadata \n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\")\n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Plot the image with matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels] (HWC)\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transform the data\n",
    "It's important that your custom data going into the model is prepared in the same way as the original training data that went into the model. Two ways to do it:\n",
    "1. Manually create the same transformation as used in the pre-trained model.\n",
    "2. Automatically get the transofrmation used in the pre-trained model.\n",
    "    - `EfficientNet_B0_Weights` is the model architecture weights we'd like to use\n",
    "\n",
    "Prior to torchvision v0.13+, to create a transform for a pretrained model in torchvision.models, the documentation stated:\n",
    "\n",
    ">All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\n",
    "\n",
    "> The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
    "manual_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "])\n",
    "\n",
    "# Get transforms from pre-trained model (required for torchvision >= 0.13)\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "auto_transform = weights.transforms()\n",
    "\n",
    "print(f\"Shape of image before transform: {img_as_array.shape}\")\n",
    "print(f\"Shape of image after manual transform: {manual_transform(img).shape}\")\n",
    "print(f\"Shape of image after auto transform: {auto_transform(img).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths: list, transform, n=3, seed=None):\n",
    "  \"\"\"\n",
    "  Selects random images from a path of images and loads/transforms \n",
    "  them then plots the original vs the transformed version.\n",
    "  \"\"\"\n",
    "  if seed:\n",
    "    random.seed(seed)\n",
    "  random_image_paths = random.sample(image_paths, k=n)\n",
    "  for image_path in random_image_paths:\n",
    "    with Image.open(image_path) as f:\n",
    "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "      ax[0].imshow(f)\n",
    "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
    "      ax[0].axis(False)\n",
    "\n",
    "      # Transform and plot target image\n",
    "      transformed_image = transform(f).permute(1, 2, 0) # note we will need to change shape for matplotlib (C, H, W) -> (H, W, C)\n",
    "      ax[1].imshow(transformed_image)\n",
    "      ax[1].set_title(f\"Transformed\\nShape: {transformed_image.shape}\")\n",
    "      ax[1].axis(\"off\")\n",
    "\n",
    "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "plot_transformed_images(image_paths=image_path_list,\n",
    "                        transform=auto_transform,\n",
    "                        n=3,\n",
    "                        seed=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert data to pytorch dataset\n",
    "We'll do this in next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_setup import create_dataloaders\n",
    "\n",
    "\n",
    "# Create training and testing DataLoaders as well as get a list of class names\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=auto_transform, # resize, convert images to between 0 & 1 and normalize them\n",
    "    batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one sample from train_dataloader\n",
    "import numpy as np\n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(f\"train_features_batch: {train_features_batch.shape}\")\n",
    "print(f\"train_labels_batch: {train_labels_batch.shape}\")\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "rand_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[rand_idx], train_labels_batch[rand_idx]\n",
    "\n",
    "img_as_array = np.asarray(img.permute(1, 2, 0))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"{label}: {class_names[label]}\")\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label size: {label.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build or pick a pretrained model for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting to build a series of machine learning modelling experiments, it's best practice to start with a baseline model. A baseline model is a simple model you will try and improve upon with subsequent models/experiments. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.1 Pick a pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
    "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
    "\n",
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(DEVICE)\n",
    "\n",
    "model # uncomment to output (it's very long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.2 Summary of model using torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.3 Freezing the base model and changing the output layer to suit our needs\n",
    "To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output layer\n",
    "\n",
    "# Set the manual seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Get the length of class_names (one output unit for each class)\n",
    "output_shape = len(class_names)\n",
    "\n",
    "# Recreate the classifier layer and seed it to the target device\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=1280, \n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pick loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a training loop to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from scripts.engine import train\n",
    "from scripts.utils import print_train_time, plot_loss_curves\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "start_timer = timer()\n",
    "# Setup training and save the results\n",
    "results = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    device=DEVICE\n",
    ")\n",
    "end_timer = timer()\n",
    "train_time_0 = print_train_time(start_timer, end_timer, DEVICE)\n",
    "\n",
    "print(results)\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make prediction\n",
    "One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.\n",
    "\n",
    "This means we'll need to make sure our images have:\n",
    "- Same shape - If our images are different shapes to what our model was trained on, we'll get shape errors.\n",
    "- Same datatype - If our images are a different datatype (e.g. torch.int8 vs. torch.float32) we'll get datatype errors.\n",
    "- Same device - If our images are on a different device to our model, we'll get device errors.\n",
    "- Same transformations - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import pred_and_plot_single_image\n",
    "\n",
    "# Get a random list of image paths from test set\n",
    "import random\n",
    "num_images_to_plot = 3\n",
    "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \n",
    "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
    "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
    "\n",
    "# Make predictions on and plot the images\n",
    "for image_path in test_image_path_sample:\n",
    "    pred_and_plot_single_image(\n",
    "        model=model, \n",
    "        image_path=image_path,\n",
    "        class_names=class_names,\n",
    "        transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
    "        # image_size=(224, 224)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scripts.utils import eval_model_classification\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model_0_res = eval_model_classification(\n",
    "    model=model,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_score\n",
    ")\n",
    "print(model_0_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random predictions\n",
    "from scripts.utils import make_predictions\n",
    "\n",
    "# random.seed(RANDOM_SEED)\n",
    "y_preds = make_predictions(model, test_dataloader)\n",
    "\n",
    "test_data = test_dataloader.dataset\n",
    "rows, cols = 3, 3\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "for i in range(1, rows * cols + 1):\n",
    "  rand_idx = torch.randint(0, len(test_data), size=[1]).item()\n",
    "  X, y_truth = test_data[rand_idx]\n",
    "  y_truth = class_names[y_truth]\n",
    "  y_pred = class_names[y_preds[rand_idx]]\n",
    "  fig.add_subplot(rows, cols, i)\n",
    "  img_as_array = np.asarray(X.permute(1, 2, 0))\n",
    "  plt.imshow(img_as_array)\n",
    "  if y_pred == y_truth:\n",
    "    plt.title(f\"Truth: {y_truth} | Pred: {y_pred}\", c=\"g\")\n",
    "  else:\n",
    "    plt.title(f\"Truth: {y_truth} | Pred: {y_pred}\", c=\"r\")\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import make_predictions\n",
    "\n",
    "\n",
    "# Make predictions with trained model\n",
    "y_pred_tensor = make_predictions(model, test_dataloader, DEVICE)\n",
    "y_pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "test_truth = torch.cat([y for X, y in test_dataloader])\n",
    "\n",
    "# 2. Setup confusion instance and compare predictions to targets\n",
    "confmat = ConfusionMatrix(task='multiclass', num_classes=len(class_names))\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=test_truth)\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with numpy\n",
    "    class_names=class_names,\n",
    "    figsize=(10, 7)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improve through experimentation\n",
    "We won't be doing any improvements right now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and reload trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_folder = Path(\"models\")\n",
    "model_folder.mkdir(parents=True, exist_ok=True)\n",
    "model_name = \"FoodSmall101.pt\"\n",
    "model_path = model_folder / model_name\n",
    "\n",
    "model.to(device=DEVICE)\n",
    "torch.save(obj=model, f=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "loaded_model = torch.load(f=model_path)\n",
    "loaded_model.to(device=DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate loaded model\n",
    "from scripts.utils import eval_model_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "loaded_model_0_results = eval_model_classification(\n",
    "    model=loaded_model,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_score\n",
    ")\n",
    "\n",
    "loaded_model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model results are close to each other\n",
    "torch.isclose(torch.tensor(model_0_res[\"model_loss\"]),\n",
    "              torch.tensor(loaded_model_0_results[\"model_loss\"]),\n",
    "              atol=1e-02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making prediciton on a custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download custom image\n",
    "import requests\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = image_path.parent / \"04-pizza-dad.jpeg\"\n",
    "\n",
    "# Download the image if it doesn't already exist\n",
    "if not custom_image_path.is_file():\n",
    "  with open(custom_image_path, \"wb\") as f:\n",
    "    # When downloading from GitHub, need to use the \"raw\" file link\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "    print(f\"Downloading {custom_image_path}...\")\n",
    "    f.write(request.content)\n",
    "else:\n",
    "  print(f\"{custom_image_path} already exists, skipping download...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import pred_and_plot_single_image\n",
    "\n",
    "# Pred on our custom image\n",
    "pred_and_plot_single_image(\n",
    "    model=model,\n",
    "    image_path=custom_image_path,\n",
    "    class_names=class_names,\n",
    "    transform=auto_transform,\n",
    "    device=DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking\n",
    "\n",
    "Format of the input: **[batch_size, no_of_channels, height, width]**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU information\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import scripts.data_setup as data_setup\n",
    "import scripts.engine as engine\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data ready (turn into tensor)\n",
    "Our dataset is a subset of the Food101 dataset. Food101 starts 101 different classes of food and 1000 images per class (750 training, 250 testing). Our dataset starts with 3 classes of food and only 10% of the images (~75 training, 25 testing).\n",
    "\n",
    "Why do this?\n",
    "- When starting out ML projects, it's important to try things on a small scale and then increase the scale when necessary.\n",
    "- The whole point is to speed up how fast you can experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.get_data as get_data\n",
    "import os\n",
    "\n",
    "\n",
    "url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "image_path = os.path.join(os.getcwd(), \"data\", \"pizza_steak_sushi\")\n",
    "get_data.download_data(url=url, save_path_str=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check details about data\n",
    "\n",
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "    \"\"\"Walks through dir_path returning its contents.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Setup train and testing paths\n",
    "image_path = Path(image_path)\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data (randomly take some images)\n",
    "\n",
    "import random \n",
    "from PIL import Image\n",
    "\n",
    "# Set seed\n",
    "# random.seed(RANDOM_SEED)\n",
    "\n",
    "# 1. Get all image paths \n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "# 2. Pick a random image path\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# 4. Open image\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# 5. Print metadata \n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\")\n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Plot the image with matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels] (HWC)\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transform the data\n",
    "It's important that your custom data going into the model is prepared in the same way as the original training data that went into the model. Two ways to do it:\n",
    "1. Manually create the same transformation as used in the pre-trained model.\n",
    "2. Automatically get the transofrmation used in the pre-trained model.\n",
    "    - `EfficientNet_B0_Weights` is the model architecture weights we'd like to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
    "manual_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "])\n",
    "\n",
    "# Get transforms from pre-trained model (required for torchvision >= 0.13)\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "auto_transform = weights.transforms()\n",
    "\n",
    "print(f\"Shape of image before transform: {img_as_array.shape}\")\n",
    "print(f\"Shape of image after manual transform: {manual_transform(img).shape}\")\n",
    "print(f\"Shape of image after auto transform: {auto_transform(img).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths: list, transform, n=3, seed=None):\n",
    "  \"\"\"\n",
    "  Selects random images from a path of images and loads/transforms \n",
    "  them then plots the original vs the transformed version.\n",
    "  \"\"\"\n",
    "  if seed:\n",
    "    random.seed(seed)\n",
    "  random_image_paths = random.sample(image_paths, k=n)\n",
    "  for image_path in random_image_paths:\n",
    "    with Image.open(image_path) as f:\n",
    "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "      ax[0].imshow(f)\n",
    "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
    "      ax[0].axis(False)\n",
    "\n",
    "      # Transform and plot target image\n",
    "      transformed_image = transform(f).permute(1, 2, 0) # note we will need to change shape for matplotlib (C, H, W) -> (H, W, C)\n",
    "      ax[1].imshow(transformed_image)\n",
    "      ax[1].set_title(f\"Transformed\\nShape: {transformed_image.shape}\")\n",
    "      ax[1].axis(\"off\")\n",
    "\n",
    "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "plot_transformed_images(image_paths=image_path_list,\n",
    "                        transform=auto_transform,\n",
    "                        n=3,\n",
    "                        seed=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert data to pytorch dataset\n",
    "We'll do this in next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_setup import create_dataloaders\n",
    "\n",
    "\n",
    "# Create training and testing DataLoaders as well as get a list of class names\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=auto_transform, # resize, convert images to between 0 & 1 and normalize them\n",
    "    batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one sample from train_dataloader\n",
    "import numpy as np\n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(f\"train_features_batch: {train_features_batch.shape}\")\n",
    "print(f\"train_labels_batch: {train_labels_batch.shape}\")\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "rand_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[rand_idx], train_labels_batch[rand_idx]\n",
    "\n",
    "img_as_array = np.asarray(img.permute(1, 2, 0))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"{label}: {class_names[label]}\")\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label size: {label.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build or pick a pretrained model for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting to build a series of machine learning modelling experiments, it's best practice to start with a baseline model. A baseline model is a simple model you will try and improve upon with subsequent models/experiments. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.1 Pick a pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
    "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
    "\n",
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(DEVICE)\n",
    "\n",
    "# model # uncomment to output (it's very long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.2 Summary of model using torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.3 Freezing the base model and changing the output layer to suit our needs\n",
    "To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output layer\n",
    "\n",
    "# Set the manual seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Get the length of class_names (one output unit for each class)\n",
    "output_shape = len(class_names)\n",
    "\n",
    "# Recreate the classifier layer and seed it to the target device\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=1280, \n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pick loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a training loop to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from scripts.engine import train\n",
    "from scripts.utils import print_train_time, plot_loss_curves, create_writer\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "start_timer = timer()\n",
    "# Setup training and save the results\n",
    "results = train(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=5,\n",
    "                device=DEVICE,\n",
    "                writer=create_writer(\"data_10_percent\", \"effnetb0\", \"5_epochs\"))\n",
    "end_timer = timer()\n",
    "train_time_0 = print_train_time(start_timer, end_timer, DEVICE)\n",
    "\n",
    "print(results)\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 View model's results in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make prediction\n",
    "One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.\n",
    "\n",
    "This means we'll need to make sure our images have:\n",
    "- Same shape - If our images are different shapes to what our model was trained on, we'll get shape errors.\n",
    "- Same datatype - If our images are a different datatype (e.g. torch.int8 vs. torch.float32) we'll get datatype errors.\n",
    "- Same device - If our images are on a different device to our model, we'll get device errors.\n",
    "- Same transformations - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import pred_and_plot_single_image\n",
    "from pathlib import Path\n",
    "\n",
    "# Get a random list of image paths from test set\n",
    "import random\n",
    "num_images_to_plot = 3\n",
    "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \n",
    "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
    "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
    "\n",
    "# Make predictions on and plot the images\n",
    "for image_path in test_image_path_sample:\n",
    "    pred_and_plot_single_image(\n",
    "        model=model, \n",
    "        image_path=image_path,\n",
    "        class_names=class_names,\n",
    "        transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
    "        # image_size=(224, 224)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scripts.utils import eval_model_classification\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model_0_res = eval_model_classification(\n",
    "    model=model,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_score\n",
    ")\n",
    "print(model_0_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random predictions\n",
    "from scripts.utils import make_predictions\n",
    "\n",
    "# random.seed(RANDOM_SEED)\n",
    "y_preds = make_predictions(model, test_dataloader)\n",
    "\n",
    "test_data = test_dataloader.dataset\n",
    "rows, cols = 3, 3\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "for i in range(1, rows * cols + 1):\n",
    "  rand_idx = torch.randint(0, len(test_data), size=[1]).item()\n",
    "  X, y_truth = test_data[rand_idx]\n",
    "  y_truth = class_names[y_truth]\n",
    "  y_pred = class_names[y_preds[rand_idx]]\n",
    "  fig.add_subplot(rows, cols, i)\n",
    "  img_as_array = np.asarray(X.permute(1, 2, 0))\n",
    "  plt.imshow(img_as_array)\n",
    "  if y_pred == y_truth:\n",
    "    plt.title(f\"Truth: {y_truth} | Pred: {y_pred}\", c=\"g\")\n",
    "  else:\n",
    "    plt.title(f\"Truth: {y_truth} | Pred: {y_pred}\", c=\"r\")\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import make_predictions\n",
    "\n",
    "\n",
    "# Make predictions with trained model\n",
    "y_pred_tensor = make_predictions(model, test_dataloader, DEVICE)\n",
    "y_pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "test_truth = torch.cat([y for X, y in test_dataloader])\n",
    "\n",
    "# 2. Setup confusion instance and compare predictions to targets\n",
    "confmat = ConfusionMatrix(task='multiclass', num_classes=len(class_names))\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=test_truth)\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with numpy\n",
    "    class_names=class_names,\n",
    "    figsize=(10, 7)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improve through experimentation\n",
    "Let's try out the following:\n",
    "\n",
    "| Experiment number | Training Dataset | Model (pretrained on ImageNet) | Number of epochs |\n",
    "| --- | --- | --- | --- |\n",
    "| 1\t| Pizza, Steak, Sushi 10% percent | EfficientNetB0 | 5 |\n",
    "| 2\t| Pizza, Steak, Sushi 10% percent | EfficientNetB2 | 5 |\n",
    "| 3\t| Pizza, Steak, Sushi 10% percent | EfficientNetB0 | 10 |\n",
    "| 4\t| Pizza, Steak, Sushi 10% percent | EfficientNetB2 | 10 |\n",
    "| 5\t| Pizza, Steak, Sushi 20% percent | EfficientNetB0 | 5 |\n",
    "| 6\t| Pizza, Steak, Sushi 20% percent | EfficientNetB2 | 5 |\n",
    "| 7\t| Pizza, Steak, Sushi 20% percent | EfficientNetB0 | 10 |\n",
    "| 8\t| Pizza, Steak, Sushi 20% percent | EfficientNetB2 | 10 |\n",
    "\n",
    "Notice how we're slowly scaling things up. With each experiment we slowly increase the amount of data, the model size and the length of training. By the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Download required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.get_data import download_data\n",
    "\n",
    "# Download 10 percent and 20 percent training data (if necessary)\n",
    "data_10_percent_path = download_data(url=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     save_path_str=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(url=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     save_path_str=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "# Check the directories\n",
    "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Transform Datasets and create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create a transform to normalize data distribution to be inline with ImageNet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
    "                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n",
    "\n",
    "# Compose transforms into a pipeline\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Resize the images\n",
    "    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n",
    "    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test DataLoaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "    test_dir=test_dir, \n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create 20% training and test data DataLoders\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create feature extraction models\n",
    "\n",
    "> Note: Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you'll know how you'll have to prepare your input data/update the model to have the correct output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "# 1. Create an instance of EffNetB2 with pretrained weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n",
    "\n",
    "# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n",
    "# summary(model=effnetb2, \n",
    "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# ) \n",
    "\n",
    "# 3. Get the number of in_features of the EfficientNetB2 classifier layer\n",
    "print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "from scripts.utils import set_seeds\n",
    "\n",
    "# Get num out features (one for each class pizza, steak, sushi)\n",
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "# Create an EffNetB0 feature extractor\n",
    "def create_effnetb0():\n",
    "    # 1. Get the base mdoel with pretrained weights and send to target device\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(DEVICE)\n",
    "\n",
    "    # 2. Freeze the base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Set the seeds\n",
    "    set_seeds()\n",
    "\n",
    "    # 4. Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # 5. Give the model a name\n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# Create an EffNetB2 feature extractor\n",
    "def create_effnetb2():\n",
    "    # 1. Get the base model with pretrained weights and send to target device\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(DEVICE)\n",
    "\n",
    "    # 2. Freeze the base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Set the seeds\n",
    "    set_seeds()\n",
    "\n",
    "    # 4. Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # 5. Give the model a name\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create experiments and set up training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create epochs list\n",
    "num_epochs = [5, 10]\n",
    "\n",
    "# 2. Create models list (need to create a new model for each experiment)\n",
    "models = [\"effnetb0\", \"effnetb2\"]\n",
    "\n",
    "# 3. Create dataloaders dictionary for various dataloaders\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
    "                     \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from scripts.utils import save_model, create_writer\n",
    "from scripts.engine import train\n",
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=42)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each DataLoader\n",
    "for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "\n",
    "    # 4. Loop through each number of epochs\n",
    "    for epochs in num_epochs: \n",
    "\n",
    "        # 5. Loop through each model name and create a new model based on the name\n",
    "        for model_name in models:\n",
    "\n",
    "            # 6. Create information print outs\n",
    "            experiment_number += 1\n",
    "            print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "            print(f\"[INFO] Model: {model_name}\")\n",
    "            print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "            print(f\"[INFO] Number of epochs: {epochs}\")  \n",
    "\n",
    "            # 7. Select the model\n",
    "            if model_name == \"effnetb0\":\n",
    "                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            else:\n",
    "                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            \n",
    "            # 8. Create a new loss and optimizer for every model\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "            # 9. Train target model with target dataloaders and track experiments\n",
    "            train(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  test_dataloader=test_dataloader, \n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn,\n",
    "                  epochs=epochs,\n",
    "                  device=DEVICE,\n",
    "                  writer=create_writer(experiment_name=dataloader_name,\n",
    "                                       model_name=model_name,\n",
    "                                       extra=f\"{epochs}_epochs\"))\n",
    "            \n",
    "            # 10. Save the model to file so we can get back the best model\n",
    "            save_filepath = f\"06_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "            save_model(model=model,\n",
    "                       target_dir=\"models\",\n",
    "                       model_name=save_filepath)\n",
    "            print(\"-\"*50 + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 View experiments in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- DO NOT TRY THIS UNLESS REQUIRED ---------------------\n",
    "# # Upload the results to TensorBoard.dev (uncomment to try it out)\n",
    "# !tensorboard dev upload --logdir runs \\\n",
    "#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n",
    "#     --description \"Comparing results of different model size, training data amount and training time.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and reload best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model has already been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "import os\n",
    "\n",
    "# Setup the best model filepath\n",
    "best_model_path = os.path.join(os.getcwd(), \"models\", \"06_effnetb2_data_20_percent_10_epochs.pth\")\n",
    "\n",
    "# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\n",
    "best_model = create_effnetb2()\n",
    "\n",
    "# Load the saved best model state_dict()\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model file size\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\n",
    "print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to make predictions on images and plot them \n",
    "from scripts.utils import pred_and_plot_single_image\n",
    "\n",
    "# Get a random list of 3 images from 20% test set\n",
    "import random\n",
    "num_images_to_plot = 3\n",
    "test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\n",
    "test_image_path_sample = random.sample(population=test_image_path_list,\n",
    "                                       k=num_images_to_plot) # randomly select k number of images\n",
    "\n",
    "# Iterate through random test image paths, make predictions on them and plot them\n",
    "for image_path in test_image_path_sample:\n",
    "    pred_and_plot_single_image(\n",
    "        model=best_model,\n",
    "        image_path=image_path,\n",
    "        class_names=class_names,\n",
    "        transform=torchvision.models.EfficientNet_B0_Weights.DEFAULT.transforms()\n",
    "        # image_size=(224, 224)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate loaded model\n",
    "from scripts.utils import eval_model_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "best_model_results = eval_model_classification(\n",
    "    model=best_model,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_score\n",
    ")\n",
    "\n",
    "best_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import make_predictions\n",
    "\n",
    "\n",
    "# Make predictions with trained model\n",
    "y_pred_tensor = make_predictions(best_model, test_dataloader, DEVICE)\n",
    "y_pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "test_truth = torch.cat([y for X, y in test_dataloader])\n",
    "\n",
    "# 2. Setup confusion instance and compare predictions to targets\n",
    "confmat = ConfusionMatrix(task='multiclass', num_classes=len(class_names))\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=test_truth)\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with numpy\n",
    "    class_names=class_names,\n",
    "    figsize=(10, 7)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making prediciton on a custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download custom image\n",
    "import requests\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = image_path.parent / \"04-pizza-dad.jpeg\"\n",
    "\n",
    "# Download the image if it doesn't already exist\n",
    "if not custom_image_path.is_file():\n",
    "  with open(custom_image_path, \"wb\") as f:\n",
    "    # When downloading from GitHub, need to use the \"raw\" file link\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "    print(f\"Downloading {custom_image_path}...\")\n",
    "    f.write(request.content)\n",
    "else:\n",
    "  print(f\"{custom_image_path} already exists, skipping download...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import pred_and_plot_single_image\n",
    "\n",
    "# Pred on our custom image\n",
    "pred_and_plot_single_image(\n",
    "    model=best_model,\n",
    "    image_path=custom_image_path,\n",
    "    class_names=class_names,\n",
    "    transform=torchvision.models.EfficientNet_B0_Weights.DEFAULT.transforms(),\n",
    "    device=DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
